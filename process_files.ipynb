{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading documents from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import tqdm\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Jazz</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Wikipedia:Protec...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Jazz_(disambigua...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Blues</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Ragtime</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" dir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Unnamed: 0  \\\n",
       "0                 https://en.wikipedia.org/wiki/Jazz   \n",
       "1  https://en.wikipedia.org/wiki/Wikipedia:Protec...   \n",
       "2  https://en.wikipedia.org/wiki/Jazz_(disambigua...   \n",
       "3                https://en.wikipedia.org/wiki/Blues   \n",
       "4              https://en.wikipedia.org/wiki/Ragtime   \n",
       "\n",
       "                                                   0  \n",
       "0  <!DOCTYPE html>\\n<html class=\"client-nojs\" dir...  \n",
       "1  <!DOCTYPE html>\\n<html class=\"client-nojs\" dir...  \n",
       "2  <!DOCTYPE html>\\n<html class=\"client-nojs\" dir...  \n",
       "3  <!DOCTYPE html>\\n<html class=\"client-nojs\" dir...  \n",
       "4  <!DOCTYPE html>\\n<html class=\"client-nojs\" dir...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original file pixels file size MB MIME type image/jpeg http Commons Zero Public Domain Dedicationfalsefalse Click date/time view file appeared time The following wikis use file View global usage file This file contains additional information probably added digital camera scanner used create digitize If file modified original state details may fully reflect modified file'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Unnamed: 0\"] == \"https://en.wikipedia.org/wiki/File:John_Coltrane_1963.jpg\"][\"0\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(texts):\n",
    "  while len(texts) < 1500:\n",
    "    yield\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class BFSScraper():\n",
    "    def __init__(self, n_to_visit):\n",
    "        self.n_to_visit = n_to_visit\n",
    "        self.already_visited = set()\n",
    "        self.q = deque()\n",
    "        self.pages_with_error_response = {}\n",
    "        self.pages = OrderedDict() # We want to reproduce the order of visiting pages\n",
    "\n",
    "    def get_unique_n_links(self, links):\n",
    "        \"\"\"This function returns at most n yet not visited links from given list of pages\"\"\"\n",
    "        new_links = []\n",
    "        candidates_ids = np.arange(len(links)) \n",
    "        np.random.shuffle(candidates_ids) # To walk randomly\n",
    "        for candidate_id in candidates_ids: # Possibly all the links could be already visited, or we won't have n links\n",
    "            if len(new_links) > self.n_to_visit:\n",
    "                break\n",
    "            link = \"https://en.wikipedia.org\" + links[candidate_id]['href']\n",
    "            if link not in self.already_visited:\n",
    "                new_links.append(link)\n",
    "            \n",
    "        return new_links\n",
    "\n",
    "    def find_links(self, parsed_page):\n",
    "        links = parsed_page.find_all(\n",
    "            'a', attrs={'href': re.compile(r'^\\/wiki\\/(?!File)(?!Main_Page)\\w*$')})  # To get only wikipedia articles, doen't take files nor something with :, ( etc. Don't go back to main page\n",
    "        \n",
    "        links = list(set(links)) # To remove duplicates, probably not the most efficient way\n",
    "\n",
    "        return links\n",
    "\n",
    "    def process_one_link(self, link):\n",
    "        response = requests.get(link)\n",
    "        if response.status_code != 200:\n",
    "            self.pages_with_error_response[link] = response.status_code\n",
    "            return None\n",
    "        \n",
    "        parsed = bs4.BeautifulSoup(response.text)\n",
    "        found_links = self.find_links(parsed)\n",
    "        n_not_visited_links = self.get_unique_n_links(found_links)\n",
    "        content = \"\".join([p.getText() for p in parsed.find(id=\"mw-content-text\").select('p')])\n",
    "\n",
    "        self.pages[link] = {} # No OrderedDefaultDict :(\n",
    "        self.pages[link][\"content\"] = content\n",
    "        self.pages[link][\"num_of_links\"] = len(found_links)\n",
    "        self.pages[link][\"selected_links\"] = n_not_visited_links\n",
    "\n",
    "        self.already_visited.add(link)\n",
    "        return n_not_visited_links\n",
    "\n",
    "    def dummy_generator(self, n):\n",
    "        while len(self.pages) < n:\n",
    "            yield\n",
    "            \n",
    "    def generate_summary(self):\n",
    "        with open(\"summary.txt\" , 'w') as f:\n",
    "            for link, page in self.pages.items():\n",
    "                f.write(f\"{link} number of reasonable links: {page['num_of_links']}\\n\")\n",
    "                f.write(\"Visited neighbours: \\n\")\n",
    "                for neighbour in page[\"selected_links\"]:\n",
    "                    f.write(f\"\\t\\t{neighbour}\\n\")\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "    def generate_csv(self):\n",
    "        with open(\"text.csv\" , 'w', encoding=\"utf-8\") as f:\n",
    "            for link, page in self.pages.items():\n",
    "                f.write(f\"{link}, {page['content'].strip()}\\n\")\n",
    "\n",
    "    def bfs(self, starting_link, n = 1000):\n",
    "        self.q.append(starting_link)\n",
    "        for _ in (pbar := tqdm.tqdm(self.dummy_generator(n))):\n",
    "            link_to_scrap = self.q.popleft()\n",
    "            links_to_visit = self.process_one_link(link_to_scrap)\n",
    "            if links_to_visit is not None: # Succesfull scraping of this particual pages and n neighbours gathered\n",
    "                pbar.set_description(f'{len(self.pages)} sites already collected')\n",
    "                for link in links_to_visit:\n",
    "                    self.q.append(link)\n",
    "\n",
    "            sleep(random.random()*3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 sites already collected: : 32it [01:14,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "bfs_scrap = BFSScraper(n_to_visit=3)\n",
    "bfs_scrap.bfs('https://en.wikipedia.org/wiki/Jazz', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs_scrap.generate_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs_scrap.generate_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "from string import ascii_letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordnet = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stemmer(string):\n",
    "    lemmatized = wordnet.lemmatize(string)\n",
    "    words = word_tokenize(lemmatized)\n",
    "    final_words = [] # can't use set to preserve order\n",
    "    for word in words:\n",
    "        if word[0] in ascii_letters:  # heuristic about non-words\n",
    "            final_words.append(word)\n",
    "    return [word for word in final_words if word not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = [custom_stemmer(text) for text in contents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, text in zip(texts.keys(), preprocessed):\n",
    "    final[key] = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://en.wikipedia.org/wiki/Jazz</th>\n",
       "      <td>Jazz music genre originated African-American c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://en.wikipedia.org/wiki/Wikipedia:Protection_policy#semi</th>\n",
       "      <td>In circumstances pages may need protected modi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://en.wikipedia.org/wiki/Jazz_(disambiguation)</th>\n",
       "      <td>Jazz style music subgenres Jazz may also refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://en.wikipedia.org/wiki/Blues</th>\n",
       "      <td>Blues music genre musical form originated Deep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://en.wikipedia.org/wiki/Ragtime</th>\n",
       "      <td>Ragtime also spelled rag-time rag time musical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    0\n",
       "https://en.wikipedia.org/wiki/Jazz                  Jazz music genre originated African-American c...\n",
       "https://en.wikipedia.org/wiki/Wikipedia:Protect...  In circumstances pages may need protected modi...\n",
       "https://en.wikipedia.org/wiki/Jazz_(disambiguat...     Jazz style music subgenres Jazz may also refer\n",
       "https://en.wikipedia.org/wiki/Blues                 Blues music genre musical form originated Deep...\n",
       "https://en.wikipedia.org/wiki/Ragtime               Ragtime also spelled rag-time rag time musical..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.DataFrame(final, index=[0]).transpose()\n",
    "df_final.to_csv('preprocessed.csv')\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files available [here](https://drive.google.com/drive/folders/1FkuFF7tCvBj8pTVDtOtFXtfSUOH7a2vw?usp=sharing), as git doesn't support so large files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c252f30e6b77a0f45cc1bbd0a26b5e9253af7ed12036af03318e246ade66513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
